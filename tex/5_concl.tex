\section{Conclusion}

This work has proposed new algorithms that optimize Sensitivity Analysis (SA) through multi-level computation reuse. These algorithms were employed to optimize SA on a medical imaging analysis workflow, executed on large scale computation environments. Three fine-grain computation reuse algorithms were implemented, along with optimizations in order to deal with balancing, level of parallelism available and memory constraints.

The application selected for evaluating the proposed optimizations was a microscopy image analysis workflow. This workflow was chosen given its relevance \cite{rtf1,rtf2,DBLP:journals/bmcbi/KurcQWWTCNLSF15,DBLP:journals/ijhpca/SaltzTPCKKK13}, having a large sample space (around 21 trillion parameter combinations). The workflow is comprised of three stages, with the most expensive operation (segmentation) being composed of seven finer-grain tasks. On this workflow distinct SA methods were applied (MOAT and VBD) with several experiment generation methods (Section \ref{sec:methods_results}). Also, these analysis were tested on a large scale environment, running the Region Templates Framework (RTF) with at most 256 worker processes.

The RTF received two main improvements. The first is a way to easily generate workflows compatible with the RTF. This was achieved by using a descriptor file for the definition of each stage of the workflow, with a GUI to build and compose workflows based on this descriptor. These workflow compositions are performed with the assist of the Taverna Workbench \cite{taverna}, which provides an easy way to generate workflows for application experts.

Although computation reuse was an already studied strategy to reduce computational cost (Section \ref{sec:reuse_intro}), it was different from what was proposed by this work. The referenced approaches would either need a training step to be executed before the main application, which would be rather inefficient for a large scale workload such as the one used on this work; or perform computation reuse through caching methods, which would be too expensive to be employed on large scale computation environments. As such, the algorithms proposed on this work fill these limitations by performing computation reuse, in a lightweight manner.

Computation reuse was implemented and evaluated in two levels, stage-level and task-level. Stage-level computation reuse, implemented with a coarse-grain merging algorithm, was already proposed on previous works \cite{rtf1,rtf2} and re-implemented in this work. Although it already reduced the overall runtime by a large factor, some other computation reuse opportunities were unachievable through coarse-grain merging. Therefore, task-level computation reuse, implemented with fine-grain merging algorithms, was employed. One important feature of the fine-grain merging algorithms was that they could be used on top of coarse-grain merging results, augmenting their performance.

Out of the three fine-grain merging algorithms proposed, implemented and evaluated the Reuse-Tree Merging Algorithm (RTMA, Section \ref{sec:rtma}) stood out as an efficient approach. The RTMA achieved both high reuse factor (around 35\%) and low execution cost, when compared with the remaining approaches. 

% It was also shown that the only input parameter of the RTMA, $MaxBucketSize$ has a limited negative impact when its value is sub-optimal (i.e., ), or in heavily memory constraint setups.

It was identified that task balancing could be a problem if the ratio of tasks per core was low. In order to solve this problem a new approach based on the RTMA was implemented. This new approach, the Task-Balanced Reuse Tree Algorithm (TRTMA), was implemented to behave as the RTMA if the raw number of tasks is large enough that maximum parallelism is achieved, while also not degrading its performance if the tasks-per-core ratio was low. Moreover, the TRTMA was implemented with the intent to take only into consideration parallelism issues, by adjusting the $MaxBuckets$ parameter, which can be automatically chosen on runtime to optimize the application makespan while also taking the memory restrictions into consideration, thus reducing the dependency on the end user.

% parameter, thus ignoring memory restrictions. In order to make this algorithm usable in all scenarios an algorithm that select a $MaxBuckets$ value that would always result in an executable set of buckets was implemented. By executable it is meant that every bucket would fit into memory. By doing this the $MaxBucket$

All algorithms were tested at first with the MOAT and VBD SA methods in order to assert their performance on real-world applications. It was shown that even though coarse-grain merging already had great speedups (from 1.85$\times$ to 1.9$\times$), fine-grain reuse managed to improve this values, achieving aggregate speedups between 1.39X to 1.51X on top of coarse-grain merging results, amounting to speedups of up to 2.89X. However, it is worth noting that the Smart Cut Algorithm (SCA) execution cost did not scale well, making this approach unfeasible for large scale setups.

The impact of the $MaxBucketSize$ constraint on the performance of the application was also analyzed, proving that the RTMA can be employed on heavily memory-constrained environments while also achieving good speedups. Since the TRTMA algorithm was equivalent to the RTMA on regular, large scale setups, only the worst-case scenario was tested. It was shown that even on this case, the TRTMA would always follow the best-case behavior. Finally, in order to validate the existence of computation reuse opportunities in the use case applications, and therefore validate the use of the proposed algorithms as a way to improve the makespan said applications, different SA experiment generators were tested in order to verify their maximum reuse degree. It was shown that across all cases the reuse degree was high enough to justify the use of computation reuse algorithms.

As a future work other application workflows would be to studied. For those applications the extensibility and ease of generating a new workflow from scratch would be observed. Then, it would be interesting to see the impact on reuse of differently structured workflows.

Another way to further optimize the workflow execution time through computation reuse is to perform balancing of buckets not by task count, but using the actual tasks costs. This approach would yield the best result, since there is not be any other source of loss of parallelism through the imbalance of buckets. However, cost analyzing is a difficult task which requires instrumentation and monitoring of such tasks \cite{instr1, instr2, instr3}, returning an estimation of these tasks costs. As such, the performance of this task-cost balancing can only be as good as the estimative of the tasks costs.

Furthermore, by balancing the buckets by task cost the bucket sizes could be limited only by parallelism and memory restrictions. The parallelism limitation is trivial to implement, being the number of buckets at least the number of worker processes. Again, the estimation is where the difficulty lies, being this memory consumption value rather hard to be found through static analysis \cite{mem_cost}. However, a task-cost balanced, memory-limited algorithm would attain not only maximum reuse, but maximum theoretical speedup since all limitations of computation would be solved.
