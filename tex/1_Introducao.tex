\section{Introduction}

We define algorithm sensitivity analysis (SA) as the process of quantifying, comparing, and correlating output from multiple analyses of a dataset computed with variations of an analysis workflow using different input parameters \cite{sa}. This process is executed in many phases of scientific research and can be used to lower the effective computational cost of analysis on such researches, or even improve the quality of the results through parameter optimization.

The main motivation of this work is the use of image analysis workflows for whole slide tissue images analysis \cite{motiv1}, which extracts salient information from tissue images in the form of segmented objects (e.g., cells) as well as their shape and texture features. Imaging features computed by such workflows contain rich information that can be used to develop morphological models of the specimens under study to gain new insights into disease mechanisms and assess disease progression.

A concern with automated biomedical image analysis is that the output quality of an analysis workflow is highly sensitive to changes in the input parameters. As such, adaptation of SA methods and methodologies employed in other fields \cite{moat,vbd,motiv2,motiv3}, can help understanding image analysis workflows for both developers and users. In short, the benefits of SA include: (i) better assessment and understanding of the correlation between input parameters and analysis output; (ii) the ability to reduce the uncertainty / variation of the analysis output by identifying the causes of variation; and (iii) workflow simplification by fixing parameters values or removing parts of the code that have limited or negligible effect on the output.

Although the benefits of using SA are many, its use in practice is limited given the data and computation challenges associated with it. For instance, a single study using a classic method such as MOAT (Morris One-At-Time) \cite{moat} may require hundreds of runs of the image analysis workflow (sample size). The execution of a single Whole Slide Tissue Image (WSI) will extract about 400,000 nuclei on average and can take hours on a single computing node. A study at scale will consider hundreds of WSIs and compute millions of nuclei per run, which need to be compared to a reference dataset of objects to assess and quantify differences as input parameters are varied by the SA method. A single analysis at this scale using a moderate sample size with 240 parameter sets and 100 WSI would take at least three years if executed sequentially \cite{rtf1}. Given how time consuming such analysis is, there is a demand to develop mechanisms to make it feasible, such as parallel execution of tasks and computation reuse.

The information generated with a SA method is computed by executing or evaluating the same workflow as values of the parameters are systematically varied. As such, there are several parameters sets which have parameters with similar values. The workflows used on this work are hierarchical and, as such, can be broken down in routines, or fine-grained tasks. As such, it would be wasteful if one of these routines were to be executed on two or more evaluations generated by the SA method with the same parameters values and inputs. Thus, the re-executions of a given routine could reuse the results of the first execution in order to reduce the overall cost of the application.

Formally, computation reuse is the process of reusing routines or tasks results instead of re-executing them. Computation reuse opportunities arise when multiple computation tasks have the same input parameters, resulting in the same output, and thus making the re-execution of such task unnecessary. Computation reuse can also be classified by the level of abstraction of the reused tasks. Furthermore, these tasks can be combined on hierarchical workflows, with the routines and sub-routines of which they are composed by, being able to be fully or partially reused. Seizing reuse opportunities is done by a merging process, in which two or more tasks are merged together, after which the repeated or reusable portions of the merged tasks are executed only once.

Computation reuse on this work will be accomplished with the use of finer-grain tasks merging algorithms, as opposed to the already existing coarser-grain merging method implemented on the  Region Templates Framework (RTF) platform, on which all algorithms are implemented on. This platform is responsible for the distributed execution of hierarchical workflows in large-scale computation environments.

% , also abstracting dependency resolution and data management. It is important to highlight that the RTF already implements a coarser-grain reuse algorithm, and that the new merging algorithms proposed here improve the performance of SA applications alongside the existing coarse-grain merging algorithm.

Other works have studied computation reuse as a means to reduce overall computational cost in different ways \cite{reuse1,reuse2,reuse3,reuse4,reuse5,reuse6,reuse7,reuse8}. Although the principle of computation reuse is rather abstract, its implementation on this work is distinct from existing methods. Some of these methods resort to hardware implementations \cite{reuse2,reuse3}, which are not general or flexible enough for the given problem. Some apply reuse by profiling the application \cite{reuse4}, which is also impracticable on the SA domain. Finally, most of them rely on caching systems of distinct levels of abstraction to reduce the overall cost of the applications \cite{reuse5,reuse6,reuse7,reuse8}, being too expensive to employ on the desired scale of distribution. 

% Given that none of the previous approaches could perform reasonably well on the desired problem, any direct comparison would be at least unfair.

Summarizing, this work focuses on two ways of accomplishing computation reuse in SA applications for the RTF: (i) coarse-grain tasks reuse and (ii) fine-grain tasks reuse. The main differences between them, besides the granularity of the tasks to be reused, are the underlying restrictions of the system used to execute these tasks. The reuse of coarse-grain tasks can offer a greater speedup when reuse happens, but there are less reuse opportunities. With fine-grain tasks these reuse opportunities are more frequent, however, more sophisticated strategies need to be employed in order to deal with dependency resolution and to avoid performance degradation due to the impact of excessive reuse to the parallelism.

%The latter case occurs when the reuse opportunities are so frequent that by taking advantage of all of them a load unbalance takes

\subsection{The Problem}

Because of high computing demands, sensitivity analysis applied to microscopy image analysis is unfeasible for routinely use when applied to whole slide tissue images.

\subsection{Contributions}

This work focuses on improving the performance of SA studies in microscopy image analysis through the application of finer-grain computation reuse on top of the coarse-grain computation reuse.

% \subsection{Specific Goals}

% This section enumerates the specific goals and presents references for those that have already been achieved in this work.

The specific contributions of this work are presented below with a reference to the section in which they are described:

\begin{enumerate}
  \item A graphical user interface for simplifying the deployment of workflows for the RTF, which is coupled with a code generator that allows the flexible use of the RTF on distinct domains [Section \ref{sec:improve}];
  
  \item The development and analysis of multi-level reuse algorithms:
  \begin{enumerate}
    \item A coarse-grain merging algorithm was implemented [Section \ref{sec:stage-merging}];
    \item A fine-grain Na\"ive Merging Algorithm was proposed and implemented [Section \ref{sec:naive-merging}];
    \item The fine-grain Smart Cut Merging Algorithm was proposed and implemented [Section \ref{sec:sca}];
    \item The fine-grain Reuse-Tree Merging Algorithm was proposed and implemented [Section \ref{sec:rtma}];
  \end{enumerate}
  
  \item Proposal and implementation of the Task-Balanced Reuse-Tree Merging Algorithm that reduces the issue of loss of parallelism due to load imbalance provoked by the Reuse-Tree Merging Algorithm [Section \ref{sec:TRTMA}];
    % \item Memory constraint problems were evaluated and dealt with {\bf [Section \ref{sec:}] Essa é aquela estratégia de execução com o valor correto de MaxBuckets para não estourar a memória. Coloca no texto?};
  
  \item The performance gains of the proposed algorithms with a real-world microscopy image analysis application were demonstrated using different SA strategies (e.g MOAT and VBD) at different scales.

  %\item This work was already partially published on the IEEE Cluster 2017 Conference, with only the Reuse-Tree Merging Algorithm presented. The full extent of this work will be submitted for a periodic publication, which is currently being drafted.
  
\end{enumerate}

The contributions of Sections \ref{sec:improve}, \ref{sec:stage-merging} and \ref{sec:rtma} were published on the IEEE Cluster 2017 conference \cite{barreiros_parallel_2017}, comprising, but not restricted to the proposal of multi-level computational reuse, the proposal of the Reuse-Tree structure for fine-grain merging and the experimental results for lower scale tests. Moreover, the contributions of Section \ref{sec:TRTMA} are currently being drafted for a submission for a journal publication. These contributions, which are an extension of the published work \cite{barreiros_parallel_2017}, includes some further analysis of computation reuse within the scope of scalability under more challenging settings, some limitations of the previously proposed solution for fine-grain merging, and finally, a new approach to cope with such limitations.

\subsection{Document Organization}

The next section describes the motivating application, the theory behind computation reuse and the Region Templates Framework (RTF), which was used to deploy the application on a parallel machine and is also the tool in which the merging algorithms were incorporated. After these considerations more relevant related work is analyzed. Section III describes the proposed solutions for multi-level computation reuse, their implementations and optimizations. On Section IV the experimental procedures are described and the results are analyzed. Finally, Section V closes this work with contributions and possible future goals for its continuation.


